{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "955deb52b02eec3e16e61df584232f2cc045079d697f475cb65582dbfebe300c"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork928-2022-01-01\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# **Data Wrangling Lab**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Estimated time needed: **45 to 60** minutes\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this assignment you will be performing data wrangling.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Objectives\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this lab you will perform the following:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "-   Identify duplicate values in the dataset.\n\n-   Remove duplicate values from the dataset.\n\n-   Identify missing values in the dataset.\n\n-   Impute the missing values in the dataset.\n\n-   Normalize data in the dataset.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Hands on Lab\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Import pandas module.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Load the dataset into a dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Read Data</h2>\n<p>\nWe utilize the <code>pandas.read_csv()</code> function for reading CSV files. However, in this version of the lab, which operates on JupyterLite, the dataset needs to be downloaded to the interface using the provided code below.\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The functions below will download the dataset into your browser:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from pyodide.http import pyfetch\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "To obtain the dataset, utilize the download() function as defined above:  \n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "await download(file_path, \"m1_survey_data.csv\")\nfile_name=\"m1_survey_data.csv\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Utilize the Pandas method read_csv() to load the data into a dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv(file_name)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "> Note: This version of the lab is working on JupyterLite, which requires the dataset to be downloaded to the interface.While working on the downloaded version of this notebook on their local machines(Jupyter Anaconda), the learners can simply **skip the steps above,** and simply use the URL directly in the `pandas.read_csv()` function. You can uncomment and run the statements in the cell below.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Finding duplicates\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this section you will identify duplicate values in the dataset.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " Find how many duplicate rows exist in the dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Find duplicate rows\nduplicates = df.duplicated()\n\n# Count the number of duplicate rows\nnum_duplicates = duplicates.sum()\n\nprint(f\"The number of duplicate rows in the dataframe is: {num_duplicates}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Removing duplicates\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Remove the duplicate rows from the dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = df.drop_duplicates()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Verify if duplicates were actually dropped.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "original_num_duplicates = df.duplicated().sum()\ndf = df.drop_duplicates()\nnew_num_duplicates = df.duplicated().sum()\nprint(f\"Number of duplicates after removing: {new_num_duplicates}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m original_num_duplicates \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[1;32m      3\u001b[0m new_num_duplicates \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "## Finding Missing values\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Find the missing values for all columns.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "missing_values = df.isnull()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Find out how many rows are missing in the column 'WorkLoc'\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "missing_in_workloc = df['WorkLoc'].isnull().sum()\nprint(f\"The number of rows with missing values in 'WorkLoc' is: {missing_in_workloc}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Imputing missing values\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Find the  value counts for the column WorkLoc.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "value_counts_workloc = df['WorkLoc'].value_counts()\nprint(value_counts_workloc)\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Identify the value that is most frequent (majority) in the WorkLoc column.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "most_frequent_workloc = df['WorkLoc'].mode()\nprint(f\"The most frequent value in 'WorkLoc' is: {most_frequent_workloc[0]}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Impute (replace) all the empty rows in the column WorkLoc with the value that you have identified as majority.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Find the most frequent value in 'WorkLoc', excluding missing values\nmost_frequent_workloc = df['WorkLoc'].dropna().mode()[0]\n\n# Replace missing values in 'WorkLoc' with the most frequent value\ndf['WorkLoc'].fillna(most_frequent_workloc, inplace=True)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "After imputation there should ideally not be any empty rows in the WorkLoc column.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Verify if imputing was successful.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Check for missing values in 'WorkLoc' after imputation\nmissing_values_after_imputation = df['WorkLoc'].isnull().sum()\nprint(f\"Number of missing values in 'WorkLoc' after imputation: {missing_values_after_imputation}\")\n# Verify the most frequent value in 'WorkLoc' after imputation\nmost_frequent_after_imputation = df['WorkLoc'].mode()[0]\nprint(f\"The most frequent value in 'WorkLoc' after imputation is: {most_frequent_after_imputation}\")\n# Count the occurrences of the most frequent value after imputation\ncount_of_most_frequent = df['WorkLoc'].value_counts(normalize=True).max()\nprint(f\"The proportion of the most frequent value in 'WorkLoc' after imputation: {count_of_most_frequent}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Normalizing data\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There are two columns in the dataset that talk about compensation.\n\nOne is \"CompFreq\". This column shows how often a developer is paid (Yearly, Monthly, Weekly).\n\nThe other is \"CompTotal\". This column talks about how much the developer is paid per Year, Month, or Week depending upon his/her \"CompFreq\". \n\nThis makes it difficult to compare the total compensation of the developers.\n\nIn this section you will create a new column called 'NormalizedAnnualCompensation' which contains the 'Annual Compensation' irrespective of the 'CompFreq'.\n\nOnce this column is ready, it makes comparison of salaries easy.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "List out the various categories in the column 'CompFreq'\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "comp_freq_categories = df['CompFreq'].unique()\nprint(comp_freq_categories)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Create a new column named 'NormalizedAnnualCompensation'. Use the hint given below if needed.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Double click to see the **Hint**.\n\n<!--\n\nUse the below logic to arrive at the values for the column NormalizedAnnualCompensation.\n\nIf the CompFreq is Yearly then use the exising value in CompTotal\nIf the CompFreq is Monthly then multiply the value in CompTotal with 12 (months in an year)\nIf the CompFreq is Weekly then multiply the value in CompTotal with 52 (weeks in an year)\n\n-->\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Method 1: Using apply() with a lambda function\ndf['NormalizedAnnualCompensation'] = df.apply(\n    lambda row: row['CompTotal'] if row['CompFreq'] == 'Yearly' \n           else row['CompTotal'] * 12 if row['CompFreq'] == 'Monthly' \n           else row['CompTotal'] * 52 if row['CompFreq'] == 'Weekly' else None,\n    axis=1\n)\n\n#Method 2: Using conditional expressions (more efficient)\ndf['NormalizedAnnualCompensation'] = (\n    df['CompFreq'].map({'Yearly': lambda x: x, 'Monthly': lambda x: x * 12, 'Weekly': lambda x: x * 52})('CompTotal')\n)\n\n#Method 3: Using np.select() (vectorized approach)\n# Define conditions and corresponding calculations\nconditions = [\n    df['CompFreq'] == 'Yearly',\n    df['CompFreq'] == 'Monthly',\n    df['CompFreq'] == 'Weekly'\n]\n\n# Define choices for the conditions\nchoices = [\n    df['CompTotal'],  # If Yearly, use CompTotal directly\n    df['CompTotal'] * 12,  # If Monthly, multiply by 12\n    df['CompTotal'] * 52  # If Weekly, multiply by 52\n]\n\n# Use np.select to assign values based on conditions\ndf['NormalizedAnnualCompensation'] = np.select(conditions, choices, default=np.nan)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ramesh Sannareddy\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Other Contributors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Rav Ahuja\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Change Log\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "| Date (YYYY-MM-DD) | Version | Changed By        | Change Description                 |\n| ----------------- | ------- | ----------------- | ---------------------------------- |\n| 2020-10-17        | 0.1     | Ramesh Sannareddy | Created initial version of the lab |\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " Copyright © 2020 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork928-2022-01-01&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBM-DA0321EN-SkillsNetwork-21426264&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ).\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# lambda\n在Python中，`lambda` 函数是一种小型的匿名函数，它允许你定义一个函数，而不需要按照标准的 `def` 关键字来定义函数名和函数体。`lambda` 函数通常用于需要一个函数对象的场合，但又不想去正式定义一个完整的函数。\n\n### 基本语法：\n\n`lambda arguments: expression`\n\n-   **arguments**：这是函数的参数，可以有多个，用逗号隔开。\n-   **expression**：这是函数的返回值，是一个表达式，而不是一个语句块。\n\n### 例子：\n\n`# 定义一个 lambda 函数，计算两个数的和 add = lambda x, y: x + y \n# 使用这个 lambda 函数 result = add(5, 3) # 输出将是 8 print(result)`\n\n在这个例子中，`lambda x, y: x + y` 创建了一个匿名函数，它接受两个参数 `x` 和 `y`，并返回它们的和。然后我们将这个 lambda 函数赋值给变量 `add`，之后可以通过调用 `add` 来使用这个函数。\n\n### 使用场景：\n\n1.  **作为参数传递**：当你需要传递一个函数作为参数时，比如 `map()`、`filter()`、`sorted()` 等。\n2.  **简化代码**：当函数体非常简单时，使用 lambda 可以减少代码量。\n3.  **临时使用**：当需要一个只在当前位置使用一次的函数时。\n\n### 例子使用 `map()`：\n\n`numbers = [1, 2, 3, 4, 5] squared = list(map(lambda x: x**2, numbers)) print(squared) # 输出将是 [1, 4, 9, 16, 25]`\n\n在这个例子中，我们使用 `map()` 函数和 lambda 表达式来将列表 `numbers` 中的每个元素平方。\n\n### 注意事项：\n\n-   Lambda 函数不能包含命令，它们只包含一个表达式。\n-   虽然 lambda 函数可以用于创建小型和简单的函数，但它们不适合复杂的逻辑。\n\nLambda 函数是Python中实现函数式编程范式的一种方式，它们提供了一种快速定义和使用函数的方法。",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# np.select\n`np.select` 是 NumPy 库中的一个函数，它基于条件数组来选择数据。这个函数通常用于基于多个条件从不同的数据源中选择值，并返回一个单一的数组。\n\n### 基本语法：\n\n`np.select(condlist, choicelist, default=0)`\n\n-   **condlist**：一个由布尔数组组成的元组或列表。每个数组对应一个选择条件。\n-   **choicelist**：一个由数组或标量组成的序列，表示当相应的条件为真时应该选择的值。`choicelist` 中的每个元素对应 `condlist` 中的一个条件。\n-   **default**：当所有条件都不满足时的默认值。如果未指定，默认值为0。\n\n### 工作原理：\n\n`np.select` 函数按顺序检查每个条件数组。一旦找到第一个为真的条件，就选择对应的值（或数组）。如果没有任何条件为真，就返回 `default` 指定的值。\n\n### 例子：\n\n`import numpy as np \n# 定义条件 conditions = [np.array([True, False, True, False]),  np.array([False, True, False, True])] \n# 定义选择 choices = [np.array([1, 2, 3, 4]),  np.array([5, 6, 7, 8])] \n# 使用 np.select selected = np.select(conditions, choices) \nprint(selected) # 输出将是 [1 6 3 8]`\n\n在这个例子中，`np.select` 首先检查第一个条件数组 `[True, False, True, False]`。对于每个索引位置，如果条件为真，则选择第一个选择数组 `[1, 2, 3, 4]` 中的对应元素；如果第一个条件为假且第二个条件为真，则选择第二个选择数组 `[5, 6, 7, 8]` 中的对应元素。如果两个条件都为假，则默认情况下不会选择任何元素，但由于没有指定 `default` 参数，所以这里不会返回默认值。\n\n### 注意事项：\n\n-   `condlist` 中的条件数组长度必须相同。\n-   `choicelist` 中的选择数组或标量数量必须与条件数组的数量一致。\n-   如果 `condlist` 中的所有条件都不满足，结果将由 `default` 参数决定。\n\n`np.select` 是一种高效的方式来根据多个条件从不同的选项中选择数据，特别是当你有多个条件需要评估，并且想要快速得到一个结果数组时。",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}